# NLP-Disaster-Posts

Это задание с Kaggle: https://www.kaggle.com/competitions/nlp-getting-started/overview

Если коротко, то по тексту поста надо определить, идет ли речь о реальной катастрофе или же нет (например, "небо в огне" может быть метафорой красивого заката).

Дополнительные цели в данном проекте: 
- сравнить работу нового оптимизатора LION с широко используемым в NLP оптимизатором AdamW,
- собрать ансамбль из трансформеров (поработать с бибилиотекой ensemble_transformers).
При этом планируется минимальное использование ресурсов: GPU T4.

Результаты исследований в ноутбуке `TransformersEnsemble.ipynb`.

Датасет с соответствующего соревнования Kaggle в файле `train.csv` , а также по ссылке https://www.kaggle.com/competitions/nlp-getting-started/data

Что касается оптимизатора LION, то, действительно, он заметно уступает AdamW при маленьких  размерах batch (менее 64). На самом деле об этом пишут создатели данного оптимизатора в своей статье https://arxiv.org/pdf/2302.06675.pdf . А я в своем исследовании в этом убедилась. LION эффективно использовать при размере батч не менее 64, а оптимально при - 4096.

Вцелом исследование состоит из следующих частей:

- предобработка датасета: очищение от хэштегов, url и тп. Кроме того, датасет содержал по несколько одинаковых сообщений, но при этом данные сообщения имели разную целевую переменную. Для таких случаев дубли удалялись, а целевая переменная исчислялась исходя из среднего значения соответсвующих дублей: если среднее было больше 0.51, то целевой переменной присваивалось значение 1; если же среднее было < 0.49, то значению целевой переменной присваивался 0. В других случаях данный пост удалялся из датасета.
- разделение тренировочной выборки на тестовую, валидационную и тренировочную.  
- выбор базовых encoder-only трансформеров и соответствующих токенизаторов: BERT (как классика), DistillRoBERTa (для оптимизации дообучения использовалась дистиллированная версия RoBERTa) и DistillRoBERTa, дообученная определять фейковые новости (тк, вероятно, это модель должна подойти под основную задачу).
- токенезация тренировочной и валидационной выборки.
- дообучение выбранных базовых моделей и проверка метрик (особое внимаение к F1-score) на тестовом датасете.
- и наконец сбор ансамбля из дообученных моделей и проверка метрик на тестовом датасете.

Итоговый F1-score составил 83%.

Что не сработало: не сработала заморозка слоев базовых трансформеров, а дообучение только классификатора. При таком подходе accuracy на валидационной выборке не превышало 55%. 

Несмотря на то, что основные цели проекта были достигнуты (работа с LION,  ensemble_transformers, используя при этом только Т4) ключевую метрики, конечно можно улучшить. Скорее всего сработает следующее:

- аугментация тренировочной выборки за счет перевода на другие языки, так как используемый датасет совсем небольшой для дообучения трансформера,
- использование более мощной GPU, чтобы можно было увеличить размер batch хотя бы до 64, чтобы можно было использовать более эффективный оптимизатор и увеличить количество эпох для дообучения),
- также использование оригинальных (а не дистиллированных версий базовых трансформеров) также может дать прирост по метрики (хотя если речь пойдте об использовании в проме, то тогда конечно будут нужны дистиллированные версии, тем более если это будет ансамбль трансформеров).




